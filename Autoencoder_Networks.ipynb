{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqfkvEmgg5NU"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sn\n",
        "import time\n",
        "import sys\n",
        "######################################################################################\n",
        "######################################################################################\n",
        "################################    Question 1     #####################################\n",
        "######################################################################################\n",
        "######################################################################################\n",
        "\n",
        "class Autoencoder_NN(object):\n",
        "    \"\"\"\n",
        "    Autoencoder class for Question 1\n",
        "    class for creating an autoencoder neural network, which is a type of neural network that learns to reconstruct \n",
        "    it inputs by learning a compressed representation of the data through the encoding part of the network and \n",
        "    then learning to reconstruct the original data from this compressed representation through the decoding part \n",
        "    of the network.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def init_coeff(self, Lin, Lhid):\n",
        "        \"\"\"\n",
        "        The method takes in two arguments: Lin which is the size of the input layer and Lhid which is the size of the hidden layer.\n",
        "        The method initializes the weights using a uniform distribution with a range \n",
        "        that is calculated using the formula sqrt(6 / (Lin + Lhid)). The method also initializes the biases for the hidden and output \n",
        "        layers using the same formula.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        L = Lin\n",
        "\n",
        "        r = np.sqrt(6 / (Lin + Lhid))\n",
        "        W1 = np.random.uniform(-r, r, size=(Lin, Lhid))\n",
        "        b1 = np.random.uniform(-r, r, size=(1, Lhid))\n",
        "\n",
        "        r = np.sqrt(6 / (Lhid + L))\n",
        "        W2 = W1.T\n",
        "        b2 = np.random.uniform(-r, r, size=(1, L))\n",
        "\n",
        "        Mul = (W1, W2, b1, b2)\n",
        "        tMul = (0, 0, 0, 0)\n",
        "\n",
        "        return Mul, tMul\n",
        "\n",
        "\n",
        "    def train(self, data, coeff, eta=0.1, alpha=0.9, epoch=10, batch_size=None):\n",
        "        \"\"\"\n",
        "       data: the training data\n",
        "       \n",
        "       coeff: a dictionary containing the required parameters for the autoencoder\n",
        "       \n",
        "       eta: the learning rate, which determines how fast the model updates its weights\n",
        "       \n",
        "       alpha: the momentum multiplier, which determines how much the model should consider \n",
        "       its past weight updates when making new updates\n",
        "       \n",
        "       epoch: the number of times the model should go through the training data\n",
        "       \n",
        "       batch_size: the size of the batches to use when training the model with stochastic gradient descent (SGD). \n",
        "       If batch_size is None, then the entire training data will be used in each iteration.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        J_array = []\n",
        "        if batch_size is None:\n",
        "            batch_size = data.shape[0]\n",
        "\n",
        "        Lin = coeff[\"Lin\"]\n",
        "        Lhid = coeff[\"Lhid\"]\n",
        "        Mul, tMul = self.init_coeff(Lin,  Lhid)\n",
        "\n",
        "        iter_per_epoch = int(data.shape[0] / batch_size)\n",
        "        \"\"\"\n",
        "        function first initializes the weights and momentum term for the weights, \n",
        "        then runs through a loop for the specified number of epochs. In each epoch, \n",
        "        the training data is shuffled and divided into batches. \n",
        "\n",
        "        \"\"\"\n",
        "        for i in range(epoch):\n",
        "\n",
        "            time_start = time.time()\n",
        "\n",
        "            J_total = 0\n",
        "\n",
        "            start = 0\n",
        "            end = batch_size\n",
        "\n",
        "            p = np.random.permutation(data.shape[0])\n",
        "            data = data[p]\n",
        "\n",
        "            tMul = (0, 0, 0, 0)\n",
        "\n",
        "            for j in range(iter_per_epoch):\n",
        "\n",
        "                batchData = data[start:end]\n",
        "\n",
        "                J, Jgrad, cache = self.aeCost(Mul, batchData, coeff)\n",
        "                Mul, tMul = self.solver(Jgrad, cache, Mul, tMul, eta, alpha)\n",
        "\n",
        "                J_total += J\n",
        "                start = end\n",
        "                end += batch_size\n",
        "\n",
        "      \n",
        "\n",
        "            J_total = J_total/iter_per_epoch\n",
        "\n",
        "            print(\"Amount_loss: {:.2f} [Epoch {} of {}]\".format(J_total, i+1, epoch ))\n",
        "            J_array.append(J_total)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "        return Mul, J_array\n",
        "\n",
        "\n",
        "    def aeCost(self, Mul, data, coeff):\n",
        "        \"\"\"\n",
        "        function that calculates the cost and the gradients of the weights for an autoencoder neural network.\n",
        "        In this function, the reconstruction error is calculated as the mean squared error between the input \n",
        "        data and the output of the autoencoder, which is obtained by passing the input data through the encoder\n",
        "        and the decoder.\n",
        "        \"\"\"\n",
        "\n",
        "        N = data.shape[0]\n",
        "\n",
        "        W1, W2, b1, b2 = Mul\n",
        "\n",
        "        rho = coeff[\"rho\"]\n",
        "        beta = coeff[\"beta\"]\n",
        "        lmb = coeff[\"lmb\"]\n",
        "        Lin = coeff[\"Lin\"]\n",
        "        Lhid = coeff[\"Lhid\"]\n",
        "\n",
        "        u = data @ W1 + b1\n",
        "        h, h_drv = self.sigmoid(u)  # N x Lhid\n",
        "        # h, h_drv = self.tanh(u)  # N x Lhid\n",
        "        v = h @ W2 + b2\n",
        "        o, o_drv = self.sigmoid(v)  # N x Lin\n",
        "        # o, o_drv = self.tanh(v)  # N x Lin\n",
        "\n",
        "        rho_b = h.mean(axis=0, keepdims=True)  # 1 x Lhid\n",
        "\n",
        "        \"\"\"\n",
        "        The function also includes a regularization term, called Tikhonov regularization, \n",
        "        which is added to the cost to prevent overfitting. This term is calculated as the \n",
        "        sum of the squares of the weights, multiplied by a regularization parameter lmb.\n",
        "        Additionally, the function includes a term called the Kullback-Leibler divergence, \n",
        "        which measures the difference between the distribution of the activations of the \n",
        "        hidden layer and a target distribution with mean rho. This term is multiplied by \n",
        "        a parameter beta to control its importance in the overall cost.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        Amount_loss = 0.5/N * (np.linalg.norm(data - o, axis=1) ** 2).sum()\n",
        "        tykhonov = 0.5 * lmb * (np.sum(W1 ** 2) + np.sum(W2 ** 2))\n",
        "        kullback = rho * np.log(rho/rho_b) + (1 - rho) * np.log((1 - rho)/(1 - rho_b))\n",
        "        kullback = beta * kullback.sum()\n",
        "\n",
        "        J = Amount_loss + tykhonov + kullback\n",
        "\n",
        "        Loss = -(data - o)/N\n",
        "        tyk2 = lmb * W2\n",
        "        tyk1 = lmb * W1\n",
        "        kullback = beta * (- rho/rho_b + (1-rho)/(1 - rho_b))/N\n",
        "\n",
        "        cache = (data, h, h_drv, o_drv)\n",
        "        Jgrad = (Loss, tyk2, tyk1, kullback)\n",
        "\n",
        "        \"\"\"\n",
        "        The function returns the cost J, the gradients of the weights Jgrad, and some\n",
        "        intermediate variables stored in a cache. The gradients of the weights are \n",
        "        used to update the weights in the optimization process.\n",
        "        \"\"\"\n",
        "        return J, Jgrad, cache\n",
        "\n",
        "\n",
        "    def solver(self, Jgrad, cache, Mul, tMul, eta, alpha):\n",
        "        \"\"\"\n",
        "       function that performs gradient descent on a neural network with two layers (one hidden and one output). It takes as input:\n",
        "\n",
        "       Jgrad: a tuple containing the error gradients for the loss, tyk2, tyk1, and kullback.\n",
        "       cache: a tuple containing the data, h, h_drv, and o_drv values that are needed to compute the updates.\n",
        "       Mul: a tuple containing the weights and biases for the two layers (W1, W2, b1, b2).\n",
        "       tMul: a tuple containing the corresponding momentum terms for the weights and biases (mW1, mW2, mb1, mb2).\n",
        "       eta: the learning rate.\n",
        "       alpha: the momentum multiplier.\n",
        "        \"\"\"\n",
        "\n",
        "        W1, W2, b1, b2 = Mul\n",
        "\n",
        "        dW1 = 0\n",
        "        dW2 = 0\n",
        "        db1 = 0\n",
        "        db2 = 0\n",
        "\n",
        "        data, h, h_drv, o_drv = cache\n",
        "        Loss, tyk2, tyk1,  kullback = Jgrad\n",
        "\n",
        "        delta = Loss * o_drv\n",
        "\n",
        "\n",
        "        dW2 = h.T @ delta + tyk2\n",
        "        db2 = delta.sum(axis=0, keepdims=True)\n",
        "\n",
        "        delta = h_drv * (delta @ W2.T + kullback)\n",
        "\n",
        "        dW1 = data.T @ delta + tyk1\n",
        "        db1 = delta.sum(axis=0, keepdims=True)\n",
        "\n",
        "      \n",
        "        dW2 = (dW1.T + dW2)/2\n",
        "        dW1 = dW2.T\n",
        "\n",
        "        dMul = (dW1, dW2, db1, db2)\n",
        "\n",
        "        \"\"\"\n",
        "        The function then computes the updates for the weights and biases using \n",
        "        the error gradients and the values in the cache tuple. It then calls the\n",
        "        update function to update the weights and biases and returns the \n",
        "        updated values for Mul and tMul.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        Mul, tMul = self.update(Mul, tMul, dMul, eta, alpha)\n",
        "\n",
        "        return Mul, tMul\n",
        "\n",
        "\n",
        "    def update(self, Mul, tMul, dMul, eta, alpha):\n",
        "        \"\"\"\n",
        "       The update function takes as input:\n",
        "\n",
        "       Mul: a tuple containing the weights and biases for the two layers (W1, W2, b1, b2).\n",
        "\n",
        "       tMul: a tuple containing the corresponding momentum terms for the weights \n",
        "       and biases (mW1, mW2, mb1, mb2).\n",
        "       \n",
        "       dMul: a tuple containing the updates for the weights and biases (dW1, dW2, db1, db2).\n",
        "       \n",
        "       eta: the learning rate.\n",
        "       \n",
        "       alpha: the momentum multiplier.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        W1, W2, b1, b2 = Mul\n",
        "        dW1, dW2, db1, db2 = dMul\n",
        "        mW1, mW2, mb1, mb2 = tMul\n",
        "\n",
        "        mW1 = eta * dW1 + alpha * mW1\n",
        "        mW2 = eta * dW2 + alpha * mW2\n",
        "        mb1 = eta * db1 + alpha * mb1\n",
        "        mb2 = eta * db2 + alpha * mb2\n",
        "\n",
        "        W1 -= mW1\n",
        "        W2 -= mW2\n",
        "        b1 -= mb1\n",
        "        b2 -= mb2\n",
        "        \"\"\"\n",
        "        The function first computes the updated momentum terms for each weight \n",
        "        and bias using the formula mW = eta * dW + alpha * mW, where mW is the \n",
        "        momentum term, dW is the update for the weight, eta is the learning rate, \n",
        "        and alpha is the momentum multiplier. It then updates the weights and \n",
        "        biases using the formula W -= mW, where W is the weight and mW is the \n",
        "        corresponding momentum term. Finally, it returns the updated values \n",
        "        for Mul and tMul.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        assert (W1 == W2.T).all()\n",
        "        Mul = (W1, W2, b1, b2)\n",
        "        tMul = (mW1, mW2, mb1, mb2)\n",
        "\n",
        "        return Mul, tMul\n",
        "\n",
        "\n",
        "    def predict(self, data, We):\n",
        "        \"\"\"\n",
        "       function that performs a forward pass through a neural network with two layers (one hidden and one output). It takes as input:\n",
        "\n",
        "       data: the input data for the neural network.\n",
        "       Mul: a tuple containing the weights and biases for the two layers (W1, W2, b1, b2).\n",
        "        \"\"\"\n",
        "\n",
        "        W1, W2, b1, b2 = Mul\n",
        "\n",
        "        u = data @ W1 + b1\n",
        "        h = self.sigmoid(u)[0]\n",
        "        v = h @ W2 + b2\n",
        "        o = self.sigmoid(v)[0]\n",
        "        return o\n",
        "\n",
        "\n",
        "    def sigmoid(self, X):\n",
        "        \"\"\"\n",
        "        The function first computes the sigmoid activation for each element in X \n",
        "        using the formula a = 1 / (1 + np.exp(-X)), where np.exp is the exponentiation \n",
        "        function from the NumPy library. It then computes the derivative of the sigmoid \n",
        "        function for each element in a using the formula d = a * (1 - a). \n",
        "        Finally, it returns both a and d as a tuple.\n",
        "        \"\"\"\n",
        "        a = 1 / (1 + np.exp(-X))\n",
        "        d = a * (1 - a)\n",
        "        return a, d\n",
        "\n",
        "\n",
        "\n",
        "def normalize(X):\n",
        "\n",
        "\n",
        "    return (X - X.min())/(X.max() - X.min())\n",
        "\n",
        "\n",
        "def plot(W, name, dim1, dim2):\n",
        "    \"\"\"\n",
        "    W: a matrix of weights to be plotted.\n",
        "    name: a string containing the filename for the plot.\n",
        "    dim1: the width of the grid of images.\n",
        "    dim2: the height of the grid of images.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(dim2, dim1, figsize=(dim1, dim2), dpi=320, facecolor='w', edgecolor='k')\n",
        "    k = 0\n",
        "    for i in range(dim2):\n",
        "        for j in range(dim1):\n",
        "            ax[i, j].imshow(W[k], cmap='gray')\n",
        "            ax[i, j].axis(\"off\")\n",
        "            k += 1\n",
        "\n",
        "    fig.subplots_adjust(wspace=0.1, hspace=0.1, left=0, right=1, bottom=0, top=1)\n",
        "    fig.savefig(name + \".png\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def q1():\n",
        "    \"\"\"\n",
        "      To perform image processing on a dataset and then train an autoencoder \n",
        "    neural network on the processed data. It does the following:\n",
        "\n",
        "      Opens a file called data1.h5 using the h5py library, reads the data stored \n",
        "    in the 'data' key, and converts it to a NumPy array of type float64.\n",
        "      \n",
        "      Converts the data to grayscale using the luminosity model.\n",
        "      \n",
        "      Normalizes the grayscale data by subtracting the mean of each image, \n",
        "    clipping the data to 3 standard deviations, and normalizing the data to \n",
        "    the range [0, 1].\n",
        "      \n",
        "      Maps the data to the range [0.1, 0.9].\n",
        "      \n",
        "      Splits the data into a training set and a validation set.\n",
        "      \n",
        "      Plots 200 random images from the training set in both RGB and grayscale.\n",
        "      \n",
        "      Trains an autoencoder neural network on the training set using the train \n",
        "    function from the Autoencoder_NN class, with the specified hyperparameters.\n",
        "      \n",
        "      Normalizes the weights of the trained autoencoder and reshapes them into \n",
        "    a matrix with the same shape as the original images.\n",
        "    Calls the plot function to visualize the weights as a grid of images.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    filename = \"data1.h5\"\n",
        "    h5 = h5py.File(filename, 'r')\n",
        "    data = h5['data'][()].astype('float64')\n",
        "    h5.close()\n",
        "\n",
        "\n",
        "    # convert to grayscale using the luminosity model\n",
        "    Luminosity = 0.2126 * data[:, 0] + 0.7152 * data[:, 1] + 0.0722 * data[:, 2]\n",
        "\n",
        "    # normalize\n",
        "    assert Luminosity.shape[1] == Luminosity.shape[2]\n",
        "    dim = Luminosity.shape[1]\n",
        "    Luminosity = np.reshape(Luminosity, (Luminosity.shape[0], dim ** 2))  # flatten\n",
        "\n",
        "    Luminosity = Luminosity - Luminosity.mean(axis=1, keepdims=True)  # differentiate per image\n",
        "    std = np.std(Luminosity)  # find std\n",
        "    Luminosity = np.clip(Luminosity, - 3 * std, 3 * std)  # clip -+3 std\n",
        "    Luminosity = normalize(Luminosity)  # normalize to 0 - 1\n",
        "\n",
        "    Luminosity = 0.1 + Luminosity * 0.8  # map to 0.1 - 0.9\n",
        "    trainData = Luminosity\n",
        "\n",
        "    # plot 200 random images\n",
        "    Luminosity = np.reshape(Luminosity, (Luminosity.shape[0], dim, dim))  # reshape for imshow\n",
        "    data = data.transpose((0, 2, 3, 1))\n",
        "    Graph1, Line1 = plt.subplots(10, 20, figsize=(20, 10))\n",
        "    Graph2, Line2 = plt.subplots(10, 20, figsize=(20, 10), dpi=200, facecolor='w', edgecolor='k')\n",
        "\n",
        "    for i in range(10):\n",
        "        for j in range(20):\n",
        "            k = np.random.randint(0, data.shape[0])\n",
        "\n",
        "            Line1[i, j].imshow(data[k].astype('float'))\n",
        "            Line1[i, j].axis(\"off\")\n",
        "\n",
        "            Line2[i, j].imshow(Luminosity[k], cmap='gray')\n",
        "            Line2[i, j].axis(\"off\")\n",
        "\n",
        "    Graph1.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
        "    Graph2.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
        "    Graph1.savefig(\"q1a_rgb.png\")\n",
        "    Graph2.savefig(\"q1a_gray.2.png\")\n",
        "    plt.close(\"all\")\n",
        "\n",
        "    eta = 0.075\n",
        "    alpha = 0.85\n",
        "    epoch = 200\n",
        "    batch_size = 32\n",
        "    rho = 0.025\n",
        "    beta = 2\n",
        "    lmb = 5e-4\n",
        "    Lin = trainData.shape[1]\n",
        "    Lhid = 64\n",
        "\n",
        "    coeff = {\"rho\": rho, \"beta\": beta, \"lmb\": lmb, \"Lin\": Lin, \"Lhid\": Lhid}\n",
        "    ae = Autoencoder_NN()\n",
        "    w = ae.train(trainData, coeff, eta, alpha, epoch, batch_size)[0]\n",
        "    W = normalize(w[0]).T\n",
        "    W = W.reshape((W.shape[0], dim, dim))\n",
        "    \"\"\"\n",
        "    To demonstrate the ability of an autoencoder to learn useful features from \n",
        "    images, which can be visualized by examining the weights of the trained autoencoder.\n",
        "\n",
        "    \"\"\"\n",
        "    name = \"rho={:.2f}|beta={:.2f}|eta={:.2f}|alpha={:.2f}|lambda={}|batch={}|Lhid={}\".format(rho, beta, eta, alpha, lmb, batch_size, Lhid)\n",
        "    wdim = int(np.sqrt(W.shape[0]))\n",
        "    plot(W, name + \"weights\", wdim, wdim)\n",
        "\n"
      ]
    }
  ]
}